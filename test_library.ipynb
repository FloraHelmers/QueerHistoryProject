{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to KG Builder Notebook\n",
    "==============\n",
    "\n",
    "KG Builder is a library that we have developed to build three pipelines for knowledge creation (extracting data mainly from Wikidata). The knowledge graphs created can then be used for various purposes such as data analysis, machine learning, and knowledge management. For more information, please refer to the paper *Strategies for creating knowledge graphs to depict a multi-perspective Queer communities representation* present alongside this notebook.\n",
    "\n",
    "The three pipelines that we have created using KG Builder are:\n",
    "\n",
    "-**Pure SPARQL**\n",
    "\n",
    "-**Star Merging**\n",
    "\n",
    "-**Crawler**\n",
    "\n",
    "In this notebook, we will describe each of these pipelines in detail and provide examples of how they can be used. We will also demonstrate how to use KG Builder to clean, handle, visualize and analyse the knowledge graphs. Overall, this notebook aims to provide a comprehensive guide to using KG Builder for knowledge creation using Wikidata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing the dependencies\n",
    "\n",
    "Due to the visualization tools used, specific versions of networkx and scipy are required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GE7Uaa83wUbv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/flora/.local/lib/python3.10/site-packages (22.3.1)\n",
      "Collecting pip\n",
      "  Using cached pip-23.0.1-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.3.1\n",
      "    Uninstalling pip-22.3.1:\n",
      "      Successfully uninstalled pip-22.3.1\n",
      "Successfully installed pip-23.0.1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rdflib in /home/flora/.local/lib/python3.10/site-packages (6.2.0)\n",
      "Requirement already satisfied: pyparsing in /usr/lib/python3/dist-packages (from rdflib) (2.4.7)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from rdflib) (59.6.0)\n",
      "Requirement already satisfied: isodate in /home/flora/.local/lib/python3.10/site-packages (from rdflib) (0.6.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from isodate->rdflib) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datashader\n",
      "  Downloading datashader-0.14.4-py2.py3-none-any.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting colorcet\n",
      "  Downloading colorcet-3.0.1-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow in /usr/lib/python3/dist-packages (from datashader) (9.0.1)\n",
      "Collecting dask\n",
      "  Downloading dask-2023.3.1-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/lib/python3/dist-packages (from datashader) (2.25.1)\n",
      "Requirement already satisfied: scipy in /usr/lib/python3/dist-packages (from datashader) (1.8.0)\n",
      "Collecting xarray\n",
      "  Downloading xarray-2023.2.0-py3-none-any.whl (975 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.8/975.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyct\n",
      "  Downloading pyct-0.5.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting numba>=0.51\n",
      "  Downloading numba-0.56.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting datashape\n",
      "  Downloading datashape-0.5.2.tar.gz (76 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas in /home/flora/.local/lib/python3.10/site-packages (from datashader) (1.5.2)\n",
      "Collecting toolz\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/lib/python3/dist-packages (from datashader) (1.21.5)\n",
      "Collecting param\n",
      "  Downloading param-1.13.0-py2.py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from numba>=0.51->datashader) (59.6.0)\n",
      "Collecting llvmlite<0.40,>=0.39.0dev0\n",
      "  Downloading llvmlite-0.39.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.6/34.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/lib/python3/dist-packages (from dask->datashader) (8.0.3)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/lib/python3/dist-packages (from dask->datashader) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/flora/.local/lib/python3.10/site-packages (from dask->datashader) (21.3)\n",
      "Collecting partd>=1.2.0\n",
      "  Downloading partd-1.3.0-py3-none-any.whl (18 kB)\n",
      "Collecting cloudpickle>=1.1.1\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting fsspec>=0.6.0\n",
      "  Downloading fsspec-2023.3.0-py3-none-any.whl (145 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.4/145.4 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multipledispatch>=0.4.7\n",
      "  Downloading multipledispatch-0.6.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: python-dateutil in /usr/lib/python3/dist-packages (from datashape->datashader) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datashader) (2022.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from multipledispatch>=0.4.7->datashape->datashader) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->dask->datashader) (2.4.7)\n",
      "Collecting locket\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Building wheels for collected packages: datashape\n",
      "  Building wheel for datashape (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for datashape: filename=datashape-0.5.2-py3-none-any.whl size=59440 sha256=1448ba9e8576286fc7d8e7acfd7ea273613bfbac46783c2a12f8d8d632de7148\n",
      "  Stored in directory: /home/flora/.cache/pip/wheels/35/c6/63/a3c12ecc9fdea10a593271de5c56481b427ad4049b90a176e1\n",
      "Successfully built datashape\n",
      "Installing collected packages: toolz, param, multipledispatch, locket, llvmlite, fsspec, cloudpickle, xarray, pyct, partd, numba, datashape, dask, colorcet, datashader\n",
      "Successfully installed cloudpickle-2.2.1 colorcet-3.0.1 dask-2023.3.1 datashader-0.14.4 datashape-0.5.2 fsspec-2023.3.0 llvmlite-0.39.1 locket-1.0.0 multipledispatch-0.6.0 numba-0.56.4 param-1.13.0 partd-1.3.0 pyct-0.5.0 toolz-0.12.0 xarray-2023.2.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting networkx==2.6\n",
      "  Downloading networkx-2.6-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy==1.8.0 in /usr/lib/python3/dist-packages (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.19 in /usr/lib/python3/dist-packages (from networkx==2.6) (1.21.5)\n",
      "Requirement already satisfied: matplotlib>=3.3 in /usr/lib/python3/dist-packages (from networkx==2.6) (3.5.1)\n",
      "Requirement already satisfied: pandas>=1.1 in /home/flora/.local/lib/python3.10/site-packages (from networkx==2.6) (1.5.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.1->networkx==2.6) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/lib/python3/dist-packages (from pandas>=1.1->networkx==2.6) (2.8.1)\n",
      "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'networkx' candidate (version 2.6 at https://files.pythonhosted.org/packages/b2/87/30ed9e62d5459bbf22b335fc2d9d1faf979d44b1c55a1f0455da0afd756e/networkx-2.6-py3-none-any.whl (from https://pypi.org/simple/networkx/) (requires-python:>=3.7))\n",
      "Reason for being yanked: Need to resolve: https://github.com/networkx/networkx/pull/4967\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: networkx\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 2.8.8\n",
      "    Uninstalling networkx-2.8.8:\n",
      "      Successfully uninstalled networkx-2.8.8\n",
      "Successfully installed networkx-2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip\n",
    "!pip install rdflib\n",
    "!pip install datashader\n",
    "\n",
    "!pip install --upgrade networkx==2.6 scipy==1.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ahnde87wwuG9"
   },
   "outputs": [],
   "source": [
    "\"\"\"from rdflib import Graph\n",
    "from rdflib.extras.external_graph_libs import rdflib_to_networkx_multidigraph,rdflib_to_networkx_digraph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf\n",
    "from datashader.layout import random_layout, circular_layout, forceatlas2_layout\n",
    "from datashader.bundling import connect_edges, hammer_bundle\n",
    "\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import scipy\"\"\"\n",
    "\n",
    "import datashader.transfer_functions as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3838046122.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_305194/3838046122.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from kgbuilder/queries import *\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from queries import *\n",
    "from visuals import create_plot_graph_force_directed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure SPARQL pipeline\n",
    "\n",
    "From the site https://query.wikidata.org/, enter the following query\n",
    "```\n",
    "CONSTRUCT {\n",
    "  ?person ?pred ?obj.\n",
    "  }\n",
    "    WHERE {\n",
    "    {\n",
    "        ?person wdt:P31 wd:Q5 . #?personId is a human\n",
    "         ?person ?pred ?obj. \n",
    "        { \n",
    "            ?person wdt:P21 ?sexorgender. #?person has ?sexorgender\n",
    "            #?sexorgender is not male, female, cisgender male, cigender female, or cisgender person\n",
    "            FILTER(?sexorgender NOT IN (wd:Q6581097, wd:Q6581072, wd:Q15145778, wd:Q15145779, wd:Q1093205)). \n",
    "        } UNION {\n",
    "            ?person wdt:P91 ?sexualorientation . #?person has ?sexualorientation\n",
    "            FILTER(?sexualorientation != wd:Q1035954). #?sexualorientation is not heterosexual\n",
    "        }\n",
    "    }\n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE]\". }\n",
    "\n",
    "    }\n",
    "```\n",
    "Then download as a .csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Star Merging Pipeline\n",
    "\n",
    "This pipeline starts with a SPARQL query to Wikidata that returns a list of Wikidata item IDs that are related to the queer community.This list is then used to create a merged RDF graph using the RDF data of the nodes from the Wikidata entity URLs in ntriples format. This merged graph is then converted to a NetworkX multidigraph that is then pruned and cleaned in various ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aq4jWqh7xEFf"
   },
   "outputs": [],
   "source": [
    "from star_merging import star_merging_pipeline\n",
    "\n",
    "prune_policy=['deadend','isolated']\n",
    "G,l=star_merging_pipeline(2, query_queer_world, prune_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image=create_plot_graph_force_directed(G)\n",
    "tf.Images(image).cols(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler\n",
    "\n",
    "In contrast to the global approach of the previous pipelines, this pipeline starts with a small number of nodes and runs an iterative process to extract important nodes that represent properties of interest (potential common points) and use those to explore and discover more queer people and communities. This pipeline runs with a PageRank algorithm on an initial set of nodes. The PageRank algorithm is run multiple times with different parameters, such as the damping factor (alpha) and the number of iterations. After the PageRank algorithm has been run, the pipeline selects a certain number of the most important nodes (k_prop) and uses them to explore further. Specifically, it runs the same SPARQL query as before, but this time using the selected nodes as the property of interest. The result is a new set of nodes that are connected to the previously selected nodes through the property of interest. These new nodes are added to the original graph, and the process repeats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qCT9c1IQ-ylK",
    "outputId": "15f5b00a-ca2c-4670-dcec-739f463f907a"
   },
   "outputs": [],
   "source": [
    "from crawler import crawler_process\n",
    "\n",
    "prune_policy=['deadend','isolated']\n",
    "G,l=star_merging_pipeline(2, query_queer_world, prune_policy)\n",
    "new_G,l=crawler_process(G, 20, 1, 5, prune_policy, n_max=10, people_list=l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "tlQDe55FcWe0",
    "outputId": "d0bd9de8-71fb-4807-af76-46b87cbeb2c9"
   },
   "outputs": [],
   "source": [
    "image=create_plot_graph_force_directed(new_G)\n",
    "tf.Images(image).cols(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting the graph\n",
    "\n",
    "It was useful when too big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDZxUrHwRlw4"
   },
   "outputs": [],
   "source": [
    "from networkx.readwrite import json_graph\n",
    "import json\n",
    "\n",
    "g_json=json_graph.node_link_data(new_G)\n",
    "json.dump(g_json,open(\"graph.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation\n",
    "\n",
    "To have a pretty visualisation, we need to have less information. To do so we can choose not to represent edges or nodes or both. This choice is always arbitrary. \n",
    "\n",
    "The first representations we had was based on the pruning methods that remove dead-ends or isolated nodes. Now we propose something more precise.\n",
    "\n",
    "## Classification\n",
    "The first step in selecting the data is classifying it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'classification'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_305194/3500809788.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mclassification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'classification'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from classification import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nodes are represented by a IRI, we will use it for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = G.nodes[:10]\n",
    "nodes_df = create_nodes_DataFrame(nodes, G)\n",
    "nodes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation at smaller scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df = pd.read_csv(\"collecting_data_with_SPARQL/cocteay_wiki.csv\")\n",
    "zoomed_in_graph(nodes_df, G)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
